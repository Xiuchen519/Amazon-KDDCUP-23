eval:
  batch_size: 512
  predict_topk: 100
  test_metrics: [recall, precision, map, ndcg, mrr, hit]
  val_metrics: [mrr, recall, precision, ndcg]
  cutoff: [100, 20, 50, 10, 5]

model:
  # transformer
  embed_dim: 128
  activation: 'gelu'
  dropout_rate: 0.5
  hidden_size: 256
  head_num: 2
  layer_norm_eps: 1e-12
  layer_num: 1

train:
  batch_size: 512
  early_stop_patience: 40
  epochs: 200
  init_method: normal
  negative_count: 1
